# src/rag_pipeline.py

"""
üì¶ Task 3: Building the RAG Core Logic and Evaluation

This module builds the Retrieval-Augmented Generation (RAG) pipeline:
1. Load FAISS index and metadata
2. Embed a user query
3. Retrieve top-k relevant complaint chunks
4. Construct a prompt
5. Generate an answer using an LLM

Default LLM: google/flan-t5-base (lightweight for CPU use)
Optional: mistralai/Mistral-7B-Instruct-v0.1 (for Colab/GPU ‚Äî see comments)
"""

# ‚úÖ Imports
import os
import pickle
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import pipeline

# ‚úÖ Absolute Paths
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
vector_dir = os.path.join(project_root, 'vector_store')
index_path = os.path.join(vector_dir, 'faiss_index')
metadata_path = os.path.join(vector_dir, 'metadata.pkl')

# ‚úÖ Config
EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2"
TOP_K = 5  # Number of relevant chunks to retrieve

# ‚úÖ Step 1: Load Vector Index and Metadata
def load_vector_store():
    index = faiss.read_index(index_path)
    with open(metadata_path, "rb") as f:
        metadata = pickle.load(f)
    return index, metadata

# ‚úÖ Step 2: Initialize Embedding and Generator Models
def initialize_models():
    embedder = SentenceTransformer(EMBEDDING_MODEL_NAME)

    # ‚úÖ Lightweight LLM for CPU (no login needed)
    generator = pipeline("text2text-generation", model="google/flan-t5-base")

    # ‚ùó Optional (for Google Colab with GPU access):
    # To use Mistral 7B model instead:
    # 1. Upload your Hugging Face token to Colab:
    #    from huggingface_hub import login
    #    login(token="your_hf_token")
    # 2. Install: !pip install transformers accelerate bitsandbytes
    # 3. Replace this line:
    #    generator = pipeline("text-generation", model="mistralai/Mistral-7B-Instruct-v0.1", device_map="auto")

    return embedder, generator

# ‚úÖ Step 3: Retrieve Top-k Context Chunks
def retrieve_context(query, embedder, index, metadata, top_k=TOP_K):
    query_vector = embedder.encode([query])
    distances, indices = index.search(np.array(query_vector), top_k)

    retrieved_chunks = []
    for i in range(top_k):
        idx = indices[0][i]
        meta = metadata[idx]
        chunk_text = meta["text"]
        retrieved_chunks.append(chunk_text)

    return "\n\n".join(retrieved_chunks), [metadata[i] for i in indices[0]]

# ‚úÖ Step 4: Build Prompt for the LLM
def build_prompt(context, query):
    prompt = f"""
You are a financial analyst assistant for CrediTrust.
Your task is to answer questions about customer complaints.
Use the following retrieved complaint excerpts to formulate your answer.
If the context doesn't contain the answer, say: 'I don't have enough information.'

Context:
{context}

Question: {query}
"""
    return prompt

# ‚úÖ Step 5: Run the RAG Pipeline
def answer_query(query, embedder, generator, index, metadata, top_k=TOP_K):
    context, retrieved_meta = retrieve_context(query, embedder, index, metadata, top_k)
    prompt = build_prompt(context, query)

    # flan-t5 uses "text2text-generation" and expects prompt only
    response = generator(prompt, max_new_tokens=256)[0]["generated_text"].strip()

    return response, context, retrieved_meta

# ‚úÖ Example Usage for Evaluation
if __name__ == "__main__":
    print("\nüöÄ Running the RAG pipeline for evaluation...\n")

    # Load vector data & models
    index, metadata = load_vector_store()
    embedder, generator = initialize_models()

    # Sample test questions (can be extended to 5‚Äì10 in evaluation)
    questions = [
        "What are common issues reported about BNPL?",
        "Why are customers frustrated with credit card services?",
        "Do people complain about transfer delays?"
    ]

    for q in questions:
        print(f"üîπ Question: {q}")
        answer, context, meta = answer_query(q, embedder, generator, index, metadata)
        print(f"üß† Answer: {answer}\n")
        print(f"üìÑ Retrieved Source Sample: {meta[0]['text'][:200]}...\n")
        print("-" * 80)
